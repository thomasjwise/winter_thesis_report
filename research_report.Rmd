---
title: "Evaluating Machine Learning Techniques for Early PTSD prognosis after Trauma using synthetic data."
output: pdf_document
bibliography: bib/winter_report_ref.bib
csl: bib/apa.citation.style.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\begin{center}

\textbf{Author:}

\textit{Thomas J Wise}

\textbf{Student Number:}

\textit{6664202}  

\textbf{Programme:}

\textit{Methodology and Statistics for the Behavioural, Biomedical and Social Sciences}

\textbf{Supervisors:}  

\textit{Dr. Mirjam van Zuiden}

\textit{Prof. dr. Rens van de Schoot}  

\textbf{Proposed Journal of Publication:}

\textit{Multivariate Behavioural Research (MBR)}

\textbf{Ethics Approval}

\textit{Status Accepted: \#20-0039; Date: 09/10/2020}

\textit{Status Accepted: \#20-0140; Date: 27/10/2020}

\end{center}
\newpage

## Introduction 

One of the potential psychopathological consequences of exposure to traumatic events (accidents, mass violence, war or natural disasters) is Post-Traumatic Stress Disorder [PTSD, @breslau1991traumatic]. PTSD is characterized through an array of physical and psychological symptoms which culminate in a significant impairment to daily life [@american2013diagnostic; @augsburger2020utilization]. Condition prognosis, is not considered adequately described through the use binary diagnostic criteria. Instead though prognosis trajectories, not reliant upon diagnostic thresholds, but rather the severity and course of symptoms over time [@bonanno2013annual; @bonanno2004loss]. Current research supports the presence of four dominant trajectories which define PTSD prognosis: resilient, recovered, chronic and delayed [@bonanno2004loss; @galatzer2018trajectories; @van2018bayesian].  

Understanding these trajectories, and their predictive factors is important, given the potential effectiveness of early, targeted PTSD interventions [@kearns2012early; @freedman2020interrupting; @short2020secondary]. However, to effectively provide these targeted interventions after a traumatic event, a sufficiently validated, trajectory focused, screening instrument is required. Current research has presented several screening instruments (SPAN [@zlotnick1996validation; @meltzer1999derivation]; IES-R [@marmer1997impact]; and TSQ [@foa1993reliability]) which can predict with adequate accuracy (>0.8 Area under the Curve), PTSD diagnosis after 6 months [SPAN=0.83, IES-R=0.83, TSQ=0.82; @mouthaan2014comparing]. These however, predict whether a participant will be above the clinical threshold for PTSD, rather than their specific trajectory. Although these examples are not exhaustive, there are currently no validated trajectory focused screening instruments currently in clinical use. 

To develop such an instrument, it is essential to build a statistical model; as without it can be challenging to understand the role and function of individual predictors [@taylor2010improving]. In the present study, a statistical model is required to 1) appropriately classify individuals to their specific trajectory and 2) predict this membership using predictors which are available directly after a traumatic event. To develop this model, firstly the application of a bayesian latent growth mixture model [outlined in: @zuiden_inprep] is required to estimate the trajectories of participants in the existing data. This estimation uses a participants change in Clinician Administered PTSD Scale (CAPS) scores [@blake1995development] across multiple time points to estimate their trajectory. In the present study, this estimation is used to provide the true trajectories for which the predicted membership are compared with allowing the development of Performance Metrics. 

Secondly, predictors which will be available directly after a traumatic event (see Selection of Variables of Interest) are used as part of Machine Learning (ML) techniques to provide predictions for these trajectory memberships. The ML technique, and appropriate model derived, will than be used to develop an instrument using the most significant predictors from those selected available. 

Supervised ML techniques are used development of this statistical model, given their capacity to identify complex underlying patters within data. In particular, this research will evaluate 9 ML techniques (see Table 1). These have been demonstrated as effective in PTSD prognosis research [@ramos2020use; @galatzer2014quantitative] and across the more general prognosis fields of psychiatric [@webb2020personalized; @passos2016identifying] and physical [@kononenko2001machine; @rajkomar2019machine; @cruz2006applications; @kourou2015machine] medicine. A broad range of literature across prognosis research was evaluated to identify suitable ML techniques, to reduce the potential influence of publication bias arising through the isolated examination of only PTSD prognosis research. 

Therefore the current research will address the question: Which Machine Learning technique is most appropriate for predicting early PTSD prognosis early after Trauma, given the inherently limited sample size within the field? 

To fully address this question, several methodological issues are accounted for. Firstly, to uphold the statistical integrity of the existing data, models will be developed and tested upon synthetic simulated data. This synthetic data will use the population parameters provided by this existing data, and be generated using one of three potential simulation methods (discussed in: Data Simulation). Through the application of ML techniques on this synthetic data, rather than the existing data, this will reduce the violations to statistical integrity which result from multiple exploratory testing [@ranganathan2016common]. The use of synthetic data, generated as part of a simulation, also provides benefits to understanding the influence of sample size when comparing ML techniques. As typically in PTSD prognosis research, data is presented with inherently limited sample sizes. For example, the existing data comprises of 852 cases [@mouthaan2014comparing], whereas other research studies have examined trajectories with small-to-moderate sample sizes [min=15, max=1765, mean=343; @smid2009delayed] and the largest individual study currently proposed, being of 5,000 participants [@mclean2020aurora]. The restrictions resulting from these sample sizes, such as reduced reliability and increased cross validation errors [@varoquaux2018cross], can be overcome through the use of synthetic data. Where sample sizes can be varied, to determine the influence of the sample size on the accuracy of the model produced, to determine the optimal sample size to evaluate a specific model or the most appropriate model for a specific sample size. This specific use of synthetic data is widely supported, where prognostic and statistical models can be developed and tested during conditions when real data is unable to be used solely [@burton2006design; @ambler2002simplifying; @morris2019using]. 


![Methodological Approach Flow Diagram](<"images/thesisplan_outline_doc.png">){width=95%}

## Methodological & Analytical Approach

In the following subsections, corresponding to those in Figure 1, the methodological and analytical approach is outlined. 

### Data, Overview of the Existing Database

The TraumaTips data set [@mouthaan2014comparing] provides the population parameters for data simulation. This data is used as it accurately represent the wider PTSD population of recently traumatized individuals regarding subsequent PTSD development [@shalev2019estimating].

Collected between September 2005 and March 2009, this data was collected to investigate the prevalence and course of PTSD after traumatic injury, and compare the validity of different prognosis screening instrument of PTSD. This data consists of 852 patients who were presenting at two level-1 trauma centers in Amsterdam (NL). Patients were excluded from the existing study if their injuries were due to self-harm, they presented with an organic brain condition, current psychotic symptoms or associated disorder, bipolar disorder or depression with psychotic features, moderate to severe traumatic brain injury or held permanent residency outside of the Netherlands. Participants were assessed at 5 time points (T1; 23 days (mean), T2; 1 month, T3; 3 months, T4; 6 months and T5; 12 months), after admission to the study. Of relevance to the current study, at T1 participants completed a range of self-report scales related to PTSD and potential risk factors, in addition to PTSD screening instruments (including SPAN [@zlotnick1996validation; @meltzer1999derivation], IES-R [@marmer1997impact], TSQ [@foa1993reliability]). At T3-T5, participants completed a clinician administered semi-structured interview, CAPS, based upon the DSM-IV criteria [@blake1995development]. 

Prior to data simulation, several data cleaning methods were applied including: A) Reclassification of Variables, B) Removal of Retrospective Questionnaires, and C) Time Based-Grouping. These methods, replicated those applied in previous research [@zuiden_inprep], and make the data suitable for calculating a participants trajectory membership. Membership is calculated using a bayesian latent growth mixture model formula, applied post-simulation to the CAPS scores to provide the true membership classification for the synthetic data. 

\newpage

### A) Reclassification of Variables  

Qualitative variables were first reclassified into categorical variables, allowing their use as quantitative variables, making them more suitable for simulation and eventual application within the machine learning models. In particular, reclassification was applied to: 

- Sport participation before the traumatic event. Reclassified using @mitchell2005task classification of sports according to their static and dynamic components. 
- Employment or Profession at the time of traumatic event. Reclassified using Centraal Bureau voor de Statistiek (CBS) employment classification [@fouarge2015beroepenindeling].
- Psychotropic medication usage at time of traumatic event, or historically. Classification of medication in line with their general pharmaceutical classification.
- Personal or family history of psychiatric conditions. Reclassified using major DSM-V [@american2013diagnostic] themes.
- Relation of family members with noted psychiatric conditions. Reclassified under the broad categories of first, second or third degree relation. 

### B) Removal of Retrospective Questionnaires 

For individual cases where self-report questionnaires were reported retrospectively, this data was removed as the reliability of the data from these questionnaires is considered low. The likelihood of retrospective reporting for the self-reported PTSD screening instruments was determined through calculating the reporting date, which if different from the specified time point would support their removal. Although this increased the level of missingness within the data, it was determined that retrospective reporting would have a greater impact upon the reliability of the results and therefore the later model than the induced missingness.

### C) Time Based-Grouping

Finally, participants were regrouped according to specific time windows of the actual timing of their assessments in relation to their traumatic event. This temporal grouping was critical for the correct application of the bayesian latent growth mixture model formula, which determined trajectory membership from CAPS score [@zuiden_inprep]. Therefore, in line with these guidelines, CAPS scores were classified according to the following time points which define the time between traumatic event and CAPS assessment: T3a, 0-60 days; T3b, 61-136 days; T4, 137-273 days and T5, 274+ days. 

### Data Simulation 

Three methods for the generation of the synthetic data are used to determine how the data should be simulated for this study. The first methods is the *conventional wisdom* [@skrondal2000design], wherein variables are considered as independent, and thus simulated in isolation. 

By contrast, the second and third methods use meta-models [@skrondal2000design], assuming that observations are the product of functions between variables. Given the complexity of both the use data and the development of meta-models, the R Package OpenMX [@neale2016openmx] is used to generate the underlying structural model and simulate data accordingly. 

In the second model missing data is retained. Whilst the third model applies multiple imputation, through the R package MICE [@buuren2010mice], to address the issue of missing data. The application of multiple imputation, will allow the evaluation of the influence of missingness on the synthetic data. 

As previously mentioned, data will be simulated using a variety of sample sizes. Specifically, these will be generated both in line with the existing data, which the synthetic data is based upon (n=852) as well as in line with smaller sample sizes (n=50) and considering large sample sizes (n=1000). The use of this range, and regular intervals accordingly, will as stated demonstrate the influence of sample size as well as enable additional conclusions to be drawn. 

At each sample size, data sets will be simulated 100 times, to address the issue of potential bias in the process of simulation. Statistical tests will be applied to each individual data set before being averaged across all 100 tests forming an estimation of the true value, based on the population parameters. 

### Comparative Testing: For Data Simulation Methodology 

To conclude which method of generating synthetic data both mimics and behaves similarity during statistical testing as the existing data, a series of comparative tests are applied. It is important to note that these tests are only used as a proof of principle, validating the performance of the synthetic data, with no optimization or interpretations made regarding the prognostic modeling from the selected synthetic data. As a result, only comparisons regarding similarity are made between the existing and synthetic data. 

The planned comparative tests can be divided into two types: A) Descriptive Statistical Tests and B) Applied Statistical Tests. The first, will allow the comparison between the overall distribution of variables within existing and synthetic data sets, whereas the applied tests will confirm the similarities in behaviour of the tests when ML techniques are applied to the existing and synthetic data sets. 

The simulation technique which demonstrates the highest similarity to the existing data, will be used to simulate data in the next part of the research. 

### A) Descriptive Statistical Tests

To examine whether the synthetic data sets are similar in their distributions, data will be taken for a subset of variables, and compared on traditional descriptive statistics (mean, standard deviation, skewness). Those which are most similar, as seen through comparative statistical testing, will be considered as closest to the existing data set. 

### B) Applied Statistical Tests 

To examine how data behaves when statistical tests are applied to them, ML techniques are applied existing data, before applying the same model, using matching parameters to the synthetic data sets in turn. From this, the core measures of performance (sensitivity, specificity and Area Under the Curve) will be compared. Those synthetic data sets most similar to the existing data, will have performance measure scores which are similar (when averaged across the multiple iterations and sample sizes). Two techniques are applied, a decision tree, as these present a simple approach to classification, and Support Vector Machines (SVMs), given the large amount of literature supporting the use of SVMs in PTSD Prognosis [@ramos2020use; @galatzer2014quantitative]. 

### Selection of Variables of Interest

As this research provides a recommendation regarding the most appropriate statistical model to be used in the development of a trajectory focused screening instrument. The model should be based upon predictive variables which are available at the time of hospital admission. In the case of the existing data, these are self-reported measures collected at Time Points 1 and 2, and in particular those without the requirement of clinical assistance. These predictor variables can be categorized into 7 groups (Figure 2). This includes variables detailing a participants core demographic information (Age and Sex), their self-report measures (IES-R, SPAN, TSQ), their reported physical activity and employment information, information regarding the type and traumatic experience generally in addition to their physical and psychological health histories. 

The developed model predicts a participants trajectory, as calculated by the bayesian latent growth mixture model [@zuiden_inprep]. These trajectories are determined using a participants CAPS score over time. Therefore, the clinical scores from the CAPS interviews are selected as parts of these variables of interest. 

![Variables of Interest Summary](<"images/varofint_diagram.png">){height=50%}

### Outcome Results 

Considering that a participants true trajectory can be considered as either a categorical trajectory classification, or numeric probability of trajectory membership. When developing the predictive model, three potential outcomes can be derived through ML techniques. Considering these different potential outcomes is important, with each providing clear advantages and disadvantages both methodological and through the potential application of model. These three potential outcome paths can be summarized as: A) Considering only the trajectory class, B) Deriving the trajectory class from the probability produced, and C) Comparing only the final distribution of the population, not comparing individual cases. 

### A) Trajectory Classes Only 

This path would predict a participants trajectory only as a categorical classification. As a multinomial approach, in practice this would provide the simplest comparison through comparing the predicted categorical classification to that of the true value determined from the CAPS score. 

Although having the most direct clinical application, providing a clear indication to the user which trajectory class they present in. This is problematic, as it presents with no margin of error between trajectory classes. Meaning it is not possible to determine how close a participant is to other trajectory classes as only a single trajectory is provided. 

### B) Derived Trajectory Classes

In contrast to A, this path would predict the probability of trajectory membership, before deriving the trajectory class from the highest probability. In practice, this outcome path behaves most similarly to the calculation which derives the true trajectory, through deriving a trajectory class from the highest probability [@zuiden_inprep]. 

In addition to having the direct clinical application through providing a clear indication to the user which trajectory class they present in. This path also overcome the issue regarding the margin of error present in A, as this allows the user, or associated clinical professional to see the probability of membership into other trajectory categories. However, this also presents some potential issues regarding those who score similarly between one or more classes, for example 33% vs 32%. The impact, and ways to address these similarities is a topic for future research as without specific examples of the likelihood and conditions this occurs, only speculations can be made. 

Finally, some methodological concerns can also be raised regarding the calculation of Performance Metrics, given the comparison of continuous variables, through trajectory probability membership. Although these can still be easily calculated comparing the derived classes from the probabilities, further investigation should be made into the most effective methodology to compare these probabilities. However, the use of both categorical and continuous outcome variables, similar to those of the original trajectory calculation presents the highest degree of potential for future application. 

### C) Final Population Distribution 

Finally, rather than comparing individuals within the population, this comparison would be made only for the distribution of individuals in each category. This cumulative approach, has the least amount of clinical application, however presents the greatest potential for demonstrating population distributions in PTSD. In practice, to derive these population distributions, either path A or B could be used, before distributions are calculated accordingly. Overall, although useful for future research, this is only useful when considering application alongside one of the aforementioned paths. 



As a whole, these methods are being considered both individually and in combination, with a combination of B and C being presented as most useful through providing specific individual level detail, alongside the population wide distributions. However as specifying the exact outcome path would limit the ML techniques to only those producing a numeric result (no binary or multinomial). Until all the discussed techniques can be applied, pathway A should not be withdrawn as a suitable outcome result, with multinomial results presenting prominently in the wider prognosis research for PTSD [@ramos2020use].


### Application of Machine Learning Techniques 

To make sufficient recommendations regarding the most appropriate ML technique for deriving a statistical model. A variety of different techniques are applied which are supported in previous research across PTSD prognosis research as well as both psychiatric and wider physical medical research. Alongside this, these techniques are selected based upon their outcome results, with classification techniques providing a nominal or binary outcome and regression techniques providing numeric, continuous outcomes. This, as highlighted in the Outcome Results section, is important in the way in which the the model recommendations are made due to the results produced. Table 1 summarizes the planned techniques, their R packages as well as their outcome measures.

\newpage

Table 1: Summary of Machine Learning Techniques

+---------------------------------+--------------------------+-----------------+-----------------------------+
| **Machine Learning Technique**  | **R Package**            | **R Function**  | **Outcome Result Type**     |
+---------------------------------+--------------------------+-----------------+-----------------------------+
| Support Vector Machines (SVMs)  | e1071 [@e1071_r]         | *svm()*         | Classification              |
+---------------------------------+--------------------------+-----------------+-----------------------------+
| Support Vector Regression (SVR) | e1071 [@e1071_r]         | *svm()*         | Regression                  |  
+---------------------------------+--------------------------+-----------------+-----------------------------+
| Regression (Multivariate)       | stat [@rcore]            | *glm(), lm()*   | Regression                  |
+---------------------------------+--------------------------+-----------------+-----------------------------+
| Regression (Logistic)           | stat [@rcore]            | *glm()*         | Classification              |
+---------------------------------+--------------------------+-----------------+-----------------------------+
| K-Nearest Neighbour             | class [@classpak]        | *knn()*         | Regression / Classification |
+---------------------------------+--------------------------+-----------------+-----------------------------+
| Decision Trees (Recursive)      | tree [@treepak]          | *tree()*        | Regression / Classification |
+---------------------------------+--------------------------+-----------------+-----------------------------+
| Decision Trees (Random Forest)  | randomForest [@Rforpak]  | *randomForest()*| Regression / Classification |
+---------------------------------+--------------------------+-----------------+-----------------------------+
| Decision Trees (XGBoosted)      | xgboost [@xgboostpak]    | *xgb.train()*   | Regression / Classification |
+---------------------------------+--------------------------+-----------------+-----------------------------+
| Neural Networks                 | nnet [@classpak]         | *nnet()*        | Regression / Classification |
+---------------------------------+--------------------------+-----------------+-----------------------------+

These techniques are applied using a K-Fold cross validation (KCV) technique [@stone1974cross] to a range of different sized synthetic simulated data (n=50 to n=10000). The use of the KCV resampling technique, is completed to assist in the evaluation of the applied ML technique. Whereas the range of sample sizes, is used to investigate the influence it has upon key performance indicators (see Performance Metrics). Additionally, each ML technique will be applied across 100 different synthetic data sets of each size, to reduce the influence of random effects on the models generated. 

### Performance Metrics 


The performance metrics of sensitivity, specificity and Area under the Curve (AUC) are used when discussing model performance. These metrics were selected primarily due to their presence in the field of clinical research [@dwyer2018machine], thus making this research accessible to other researchers. But also to draw effective comparisons between previous research which has examined PTSD prognosis specifically [@ramos2020use]. These metrics will be displayed visually (as seen in figure 3), displaying both the mean and a subset of individual simulations. 

![Mock Up Results Performance Metrics](<"images/sizemodel_plot.png">){width=95%}

From this, several questions will be addressed:

- 1) Which technique has the highest overall metric (across all three measures) at the proposed Traumatips sample size (n=852).
- 2) Which technique has the highest overall metric, regardless of sample size. 
- 3) Which technique has the highest overall metric below the proposed Traumatips sample size, and how does this change as the size increases?

Through addressing these specific questions, recommendations regarding which ML technique is most appropriate for the development of this statistical model can be made. These recommendations will provide the foundation for the trajectory focused screening instrument to be developed from, using the Traumatips data set. Due to the scope of this research, it is likely that multiple methods will be recommended, however the potential advantages and disadvantages of these methods will be discussed in the recommendations made. 


\newpage

## References

